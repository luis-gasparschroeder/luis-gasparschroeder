<p align="left">
  <a href="https://luisgasparschroeder.com">
    <img src="https://img.shields.io/badge/Website-luisgasparschroeder.com-blue?style=flat-square" alt="Website">
  </a>
  <a href="https://scholar.google.com/citations?user=oLSpQiAAAAAJ&hl">
    <img src="https://img.shields.io/badge/Google%20Scholar-Profile-red?style=flat-square&logo=google-scholar" alt="Google Scholar">
  </a>
  <a href="https://www.linkedin.com/in/luis-gaspar-schroeder/">
    <img src="https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=flat-square&logo=linkedin" alt="LinkedIn">
  </a>
</p>

I'm founding member of technical staff at UniversalAGI, building ML models for physics from first principles. I researched at the UC Berkeley's Sky Computing Lab with Joseph E. Gonzalez and Matei Zaharia on efficient inference and systems for LLMs (semantic caching with error guarantees, compound AI orchestration, agentic systems, database query optimization, and sparse attention). Before, I built production systems at Snowflake and Microsoft and studied computer science at TU Munich and UC Berkeley.

<div align="left">
    <div style="background-color: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; padding: 20px 40px; margin: 20px auto; max-width: 800px;">
        <p style="margin: 0; color: #24292f; font-size: 13px; line-height: 1.6;">
        <i>How do you build efficient and reliable systems when the components underneath are probabilistic?</i>
        </p>
    </div>
</div>

## Selected Papers

<table>
<tr>
<td width="60%">

**[Optimizing LLM Queries in Relational Workloads](https://proceedings.mlsys.org/paper_files/paper/2025/hash/b5dc49f44db2fadc5c4d717c57f4a424-Abstract-Conference.html)**

Row and column reordering algorithms to maximize KV cache reuse in batch analytics with LLMs. 3.4× faster job completion, 32% cost reduction.

</td>
<td width="40%">
<img src="https://img.shields.io/badge/MLSys-2025-4CAF50?style=for-the-badge" alt="MLSys 2025">
</td>
</tr>

<tr>
<td width="60%">

**[vCache: Semantic Caching with Error Rate Guarantees](https://arxiv.org/abs/2502.03771)** | [Code](https://github.com/vcache-project/vCache)

First production-ready semantic caching with mathematical error guarantees. Outperforms all baselines on error rate and cache hit rate.

</td>
<td width="40%">
  <img src="https://img.shields.io/badge/ICLR-2026-6A5ACD?style=for-the-badge" alt="ICLR 2026">
</td>
</tr>

<tr>
<td width="60%">

**[vAttention: Dynamic Sparse Attention for Efficient Inference](https://arxiv.org/abs/2510.05688)** | [Code](https://github.com/skylight-org/sparse-attention-hub)

First practical sparse attention with mathematical accuracy guarantees. Matches full model quality at up to 20× sparsity.

</td>
<td width="40%">
  <img src="https://img.shields.io/badge/ICLR-2026-6A5ACD?style=for-the-badge" alt="ICLR 2026">
</td>
</tr>

<tr>
<td width="60%">


**[ALTO: Compound AI System Orchestration](https://arxiv.org/abs/2403.04311v3)**

Automatic optimization of compound AI systems through streaming and parallelism via nested ancestry abstraction. 10-30% latency improvements over LangGraph.

</td>
<td width="40%">
<img src="https://img.shields.io/badge/arXiv-2403.04311-B31B1B?style=for-the-badge&logo=arxiv" alt="arXiv">
</td>
</tr>

<tr>
<td width="60%">

**[The Danger of Overthinking in Agentic Systems](https://arxiv.org/abs/2502.08235)**

Identifies overthinking in reasoning models—when they favor extended reasoning over environmental interaction. Our mitigation strategies improve performance by 30%, reduce costs by 43%.

</td>
<td width="40%">
<img src="https://img.shields.io/badge/arXiv-2502.08235-B31B1B?style=for-the-badge&logo=arxiv" alt="arXiv">
</td>
</tr>
</table>

<br>
